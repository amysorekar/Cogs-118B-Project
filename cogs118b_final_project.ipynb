{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e63fed1-a8e9-4e6b-acc0-78ba3453bd2e",
   "metadata": {},
   "source": [
    "# Cogs 118b Final Project \n",
    "## Comparing and Contrasting the Performance of Different Machine Learning Models on clustering and classifying animal sounds \n",
    "\n",
    "Group Members:\n",
    "- Anand Mysorekar\n",
    "- Alex Franz\n",
    "- Jack Determan\n",
    "- Austin Blanco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66984b93",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "discuss what we tried and what we found high level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa39ca",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "discuss clustering and classification as a task and the data we used "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29981fe5",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "discuss the dataset (samples, instances, etc) and the models we used and the metrics we used to evaluate them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a8468",
   "metadata": {},
   "source": [
    "# Data Preprocessing, Feature Extraction, PCA, and Clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97ffd9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892a296-734b-4269-ba89-7a3afbd3a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from skimage.transform import resize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import SpectralClustering, KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score, normalized_mutual_info_score, homogeneity_score, \n",
    "    completeness_score, v_measure_score, classification_report, confusion_matrix\n",
    ")\n",
    "from minisom import MiniSom\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Dropout, TimeDistributed, GlobalAveragePooling2D, \n",
    "    Attention, Bidirectional\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop, AdamW\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043a412-beaa-4bdb-b26a-3ff922cd94bd",
   "metadata": {},
   "source": [
    "## Extracting Features using Librosa\n",
    "\n",
    "Some commonly used audio features for classification include:\n",
    "\n",
    "* **MFCCs (Mel-Frequency Cepstral Coefficients)**: Captures the timbral aspects of the sound, which is crucial for distinguishing sounds with similar pitches but different characteristics\n",
    "* **Spectral Features**: Describes the shape of the spectrum and provide insights into the distribution of energy across frequencies\n",
    "    * **Spectral Centroid**: Associated with brightness. Sounds with high spectral centroids (e.g., cymbals) are perceived as brighter, while sounds with low centroids (e.g., bass) are darker.\n",
    "    * **Spectral Bandwidth**: A high bandwidth means the sound contains a wide range of frequencies (e.g., noise), while a narrow bandwidth suggests a pure tone\n",
    "    * **Spectral Rolloff**: Indicates the \"tail\" of the spectrum. Useful for distinguishing tonal sounds (low rolloff) from noise-like sounds (high rolloff)\n",
    "    * **Zero-Crossing Rate**: Higher rates are associated with noisier or percussive sounds, while lower rates occur in harmonic or tonal sounds\n",
    "* **RMS Energy**: Loudness\n",
    "*  **Chroma Features**: Rate of sign changes in the waveform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221386e-550a-4b42-a5fc-8779f482acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "\n",
    "    y, sr = librosa.load(file_path, sr=None) \n",
    "\n",
    "    # MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  \n",
    "    mfccs_mean = np.mean(mfccs, axis=1)  \n",
    "\n",
    "    # Spectral features\n",
    "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "    spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
    "    zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=y))\n",
    "\n",
    "    # RMS energy\n",
    "    rms = np.mean(librosa.feature.rms(y=y))\n",
    "\n",
    "    # Combine all features into a single vector\n",
    "    features = np.hstack([\n",
    "        mfccs_mean,\n",
    "        spectral_centroid,\n",
    "        spectral_bandwidth,\n",
    "        spectral_rolloff,\n",
    "        zero_crossing_rate,\n",
    "        rms\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def process_audio_directory(directory_path):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.wav'):  \n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            \n",
    "            try:\n",
    "                features = extract_features(file_path)\n",
    "                \n",
    "                data.append([file_name] + list(features))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "    \n",
    "    mfcc_columns = [f\"mfcc_{i+1}\" for i in range(13)]\n",
    "    other_columns = [\"spectral_centroid\", \"spectral_bandwidth\", \"spectral_rolloff\", \"zero_crossing_rate\", \"rms\"]\n",
    "    columns = [\"file_name\"] + mfcc_columns + other_columns\n",
    "\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_audio_directory_recursive(directory_path):\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.wav'): \n",
    "                file_path = os.path.join(root, file_name)\n",
    "                \n",
    "                try:\n",
    "                    features = extract_features(file_path)\n",
    "                    \n",
    "                    data.append([file_name, file_path] + list(features))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_name}: {e}\")\n",
    "    \n",
    "    mfcc_columns = [f\"mfcc_{i+1}\" for i in range(13)]\n",
    "    other_columns = [\"spectral_centroid\", \"spectral_bandwidth\", \"spectral_rolloff\", \"zero_crossing_rate\", \"rms\"]\n",
    "    columns = [\"file_name\", \"file_path\"] + mfcc_columns + other_columns\n",
    "\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "def plot_and_compute_metrics(df, model, expected_clusters, expected_labels):\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(model, annot=True, fmt='d', cmap='Blues', xticklabels=expected_clusters, yticklabels=expected_labels)\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Animal Label')\n",
    "    plt.title('Counts of Animal Labels per Cluster')\n",
    "    plt.show()\n",
    "\n",
    "    ari = adjusted_rand_score(df['animal_label'], df['cluster'])\n",
    "    print(f\"Adjusted Rand Index (ARI): {ari:.2f}\")\n",
    "\n",
    "    nmi = normalized_mutual_info_score(df['animal_label'], df['cluster'])\n",
    "    print(f\"Normalized Mutual Information (NMI): {nmi:.2f}\")\n",
    "\n",
    "    homogeneity = homogeneity_score(df['animal_label'], df['cluster'])\n",
    "    print(f\"Homogeneity Score: {homogeneity:.2f}\")\n",
    "\n",
    "    completeness = completeness_score(df['animal_label'], df['cluster'])\n",
    "    print(f\"Completeness Score: {completeness:.2f}\")\n",
    "\n",
    "    v_measure = v_measure_score(df['animal_label'], df['cluster'])\n",
    "    print(f\"V-Measure Score: {v_measure:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478d788d-ca4f-48e6-aa3d-c43e55109885",
   "metadata": {},
   "outputs": [],
   "source": [
    "bear_file = \"Animal Sounds/Bear/Bear_1.wav\"\n",
    "bear_dir = \"Animal Sounds/Bear\"\n",
    "animal_dir = \"Animal Sounds\"\n",
    "animals = [\"Animal Sounds/Bear/Bear_1.wav\",\"Animal Sounds/Cat/Cat_1.wav\",\"Animal Sounds/Chicken/Chicken_1.wav\",\"Animal Sounds/Lion/Lion_1.wav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00834e04-c258-4b3b-85f8-fa5cefe0d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "bear_sample = extract_features(bear_file)\n",
    "print(bear_sample)\n",
    "\n",
    "all_data = process_audio_directory_recursive(animal_dir)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780e6e3",
   "metadata": {},
   "source": [
    "### Run PCA on the librosa-extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398f2cb-a4c4-429b-9cf5-7ce9eddf8912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric columns\n",
    "animal_features = all_data.drop(columns=[\"file_name\",\"file_path\"])  \n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "animal_scaled = scaler.fit_transform(animal_features)\n",
    "\n",
    "# Apply PCA \n",
    "pca = PCA(n_components=0.95)  \n",
    "principal_components_librosa = pca.fit_transform(animal_scaled)\n",
    "\n",
    "# Create a DataFrame for PCA results\n",
    "pca_df = pd.DataFrame(data=principal_components_librosa)\n",
    "\n",
    "print(f\"Number of components selected: {pca.n_components_}\")\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "pca_df['file_name'] = all_data['file_name'].values\n",
    "pca_df['animal_label'] = pca_df['file_name'].str.split('_').str[0]\n",
    "pca_df = pca_df.drop(columns=[\"file_name\"]) \n",
    "print(pca_df.head())\n",
    "\n",
    "# Plot the cumulative explained variance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Cumulative Explained Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label=\"95% Explained Variance\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99780cb",
   "metadata": {},
   "source": [
    "### Perform KMeans clustering on the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca76722-1857-4da0-b72b-d63ae0c882bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run K-means clustering\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "pca_df['cluster'] = kmeans.fit_predict(principal_components_librosa)\n",
    "\n",
    "# Create a contingency matrix\n",
    "contingency_matrix_k_means = pd.crosstab(pca_df['animal_label'], pca_df['cluster'])\n",
    "\n",
    "# Ensure all clusters and labels are represented\n",
    "expected_clusters = range(kmeans.n_clusters)\n",
    "expected_labels = sorted(pca_df['animal_label'].unique())\n",
    "\n",
    "# Reindex the contingency matrix to include all expected clusters and labels\n",
    "contingency_matrix_k_means = contingency_matrix_k_means.reindex(index=expected_labels, columns=expected_clusters, fill_value=0)\n",
    "print(contingency_matrix_k_means)\n",
    "\n",
    "plot_and_compute_metrics(pca_df, contingency_matrix_k_means, expected_clusters, expected_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46c9ad",
   "metadata": {},
   "source": [
    "### Perform Spectral Clustering on the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ca928-7e13-4cf6-99f9-a6822844f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Spectral Clustering\n",
    "spectral = SpectralClustering(n_clusters=10, affinity='nearest_neighbors', random_state=42)\n",
    "pca_df['cluster'] = spectral.fit_predict(principal_components_librosa)\n",
    "\n",
    "# Create a contingency matrix\n",
    "contingency_matrix_spectral = pd.crosstab(pca_df['animal_label'], pca_df['cluster'])\n",
    "\n",
    "# Ensure all clusters and labels are represented\n",
    "expected_clusters = range(9)  \n",
    "expected_labels = sorted(pca_df['animal_label'].unique())\n",
    "\n",
    "# Reindex the contingency matrix to include all expected clusters and labels\n",
    "contingency_matrix_spectral = contingency_matrix_spectral.reindex(index=expected_labels, columns=expected_clusters, fill_value=0)\n",
    "print(contingency_matrix_spectral)\n",
    "\n",
    "plot_and_compute_metrics(pca_df, contingency_matrix_spectral, expected_clusters, expected_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62170534",
   "metadata": {},
   "source": [
    "### Perform GMM clustering on the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2bbdb-059c-4a9a-b3d6-cfb0d435dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Gaussian Mixture Model (GMM)\n",
    "gmm = GaussianMixture(n_components=10, random_state=42)\n",
    "pca_df['cluster'] = gmm.fit_predict(principal_components_librosa)\n",
    "\n",
    "# Create a contingency matrix\n",
    "contingency_matrix_gmm = pd.crosstab(pca_df['animal_label'], pca_df['cluster'])\n",
    "\n",
    "# Ensure all clusters and labels are represented\n",
    "expected_clusters = range(9) \n",
    "expected_labels = sorted(pca_df['animal_label'].unique())\n",
    "\n",
    "# Reindex the contingency matrix to include all expected clusters and labels\n",
    "contingency_matrix_gmm = contingency_matrix_gmm.reindex(index=expected_labels, columns=expected_clusters, fill_value=0)\n",
    "print(contingency_matrix_gmm)\n",
    "\n",
    "plot_and_compute_metrics(pca_df, contingency_matrix_gmm, expected_clusters, expected_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b1eea",
   "metadata": {},
   "source": [
    "### Perform Self-Organizing Maps clustering on the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a10d08-150f-4e76-b349-a0fd3f4d7d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data \n",
    "scaler = StandardScaler()\n",
    "animal_scaled = scaler.fit_transform(principal_components_librosa) \n",
    "\n",
    "# Initialize the SOM\n",
    "som = MiniSom(x=10, y=10, input_len=animal_scaled.shape[1], sigma=1.0, learning_rate=0.5, random_seed=42)\n",
    "\n",
    "# Train the SOM\n",
    "som.train(animal_scaled, 1000, verbose=True)\n",
    "\n",
    "# Plot the SOM's U-matrix\n",
    "# U-matrix visualizes the distance between the neurons in the SOM grid. Larger distances mean dissimilarity.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"U-Matrix of SOM\")\n",
    "sns.heatmap(som.distance_map().T, cmap='coolwarm', cbar=False)\n",
    "plt.show()\n",
    "\n",
    "# Plot the data points on the SOM grid\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(som.distance_map().T, cmap='coolwarm', cbar=False)\n",
    "\n",
    "# Assign each data point to a neuron (winning node)\n",
    "win_map = som.win_map(animal_scaled)\n",
    "\n",
    "# We can use the same colors to show where the data points map on the SOM grid\n",
    "for label in range(len(animal_scaled)):\n",
    "    x, y = som.winner(animal_scaled[label])\n",
    "    plt.text(x, y, str(label), color='black', fontsize=10)\n",
    "\n",
    "plt.title('Data Points Mapped to SOM Grid')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the SOM clusters\n",
    "# Cluster the data points by grouping based on their winning node\n",
    "# You can assign each point a cluster label (i.e., the coordinates of the winning node)\n",
    "labels_som = np.array([som.winner(x) for x in animal_scaled])\n",
    "\n",
    "# Convert coordinates to cluster labels\n",
    "cluster_labels = np.array([f'Cluster_{x[0]}_{x[1]}' for x in labels_som])\n",
    "\n",
    "pca_df['SOM_cluster'] = cluster_labels\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=animal_scaled[:, 0], y=animal_scaled[:, 1], hue=pca_df['SOM_cluster'], palette='Set1', s=100)\n",
    "plt.title(\"Clustering on SOM Grid\")\n",
    "plt.show()\n",
    "\n",
    "# Clustering performance metrics\n",
    "ari = adjusted_rand_score(pca_df['animal_label'], pca_df['SOM_cluster'])\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.2f}\")\n",
    "\n",
    "nmi = normalized_mutual_info_score(pca_df['animal_label'], pca_df['SOM_cluster'])\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi:.2f}\")\n",
    "\n",
    "homogeneity = homogeneity_score(pca_df['animal_label'], pca_df['SOM_cluster'])\n",
    "print(f\"Homogeneity: {homogeneity:.2f}\")\n",
    "\n",
    "completeness = completeness_score(pca_df['animal_label'], pca_df['SOM_cluster'])\n",
    "print(f\"Completeness: {completeness:.2f}\")\n",
    "\n",
    "v_measure = v_measure_score(pca_df['animal_label'], pca_df['SOM_cluster'])\n",
    "print(f\"V-Measure: {v_measure:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb27573",
   "metadata": {},
   "source": [
    "### Results of Librosa Feature Extraction Clustering Results\n",
    "\n",
    "\n",
    "put evaluation metrics for each model here in a table just for easy viewing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841594f7-55d6-4345-929b-a1fe4a9579a5",
   "metadata": {},
   "source": [
    "## Processing the File using a Spectrogram \n",
    "\n",
    "talk about the spectrogram and how we used it to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180a4ca-8b49-4a87-a010-7c456b59b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrograms_grid(audio_files, rows, cols, figsize=(15, 10), cmap='viridis'):\n",
    "    \n",
    "    if rows * cols < len(audio_files):\n",
    "        raise ValueError(\"Grid size (rows * cols) is smaller than the number of audio files.\")\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=figsize)\n",
    "    axes = axes.flatten() \n",
    "\n",
    "    for i, audio_file in enumerate(audio_files):\n",
    "        y, sr = librosa.load(audio_file)\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "\n",
    "        librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log', cmap=cmap, ax=axes[i])\n",
    "        axes[i].set_title(f'Spectrogram: {audio_file}')\n",
    "        axes[i].set_xlabel('Time (s)')\n",
    "        axes[i].set_ylabel('Frequency (Hz)')\n",
    "\n",
    "    for j in range(len(audio_files), len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    fig.colorbar(plt.cm.ScalarMappable(cmap=cmap), ax=axes, format='%+2.0f dB', orientation='vertical', fraction=0.02, pad=0.04)\n",
    "    plt.show()\n",
    "\n",
    "def process_wav_directory_to_spectrogram_df(root_dir):\n",
    "   \n",
    "    data = []\n",
    "    file_paths = []\n",
    "\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                try:\n",
    "                    y, sr = librosa.load(file_path, sr=None)  # Use original sample rate\n",
    "                    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "                    S_dB = librosa.power_to_db(S, ref=np.max)  # Convert to dB scale\n",
    "                    \n",
    "                    flattened_spectrogram = S_dB.flatten()\n",
    "                    data.append(flattened_spectrogram)\n",
    "                    file_paths.append(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['file_path'] = file_paths  \n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_label(file_path):\n",
    "    \n",
    "    parts = os.path.normpath(file_path).split(os.sep)\n",
    "    return parts[1]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b8896-1478-44d6-b93e-0c01ea0e26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrograms_grid(animals,rows=2,cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2769ec37-5a0a-41a7-97a9-aebd9497222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_data = process_wav_directory_to_spectrogram_df(animal_dir)\n",
    "spectrogram_data = spectrogram_data.fillna(0)\n",
    "spectrogram_data\n",
    "\n",
    "file_path = 'Animal Sounds/Cat/Cat_17.wav'\n",
    "label = extract_label(file_path)\n",
    "print(label)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaecd31",
   "metadata": {},
   "source": [
    "### Run PCA on the Spectrogram-Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66272f6-6b88-43b7-9a46-39fe6f95d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_data_num = spectrogram_data.drop(columns=\"file_path\") \n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "spectrogram_scaled = scaler.fit_transform(spectrogram_data_num)\n",
    "\n",
    "# Apply PCA \n",
    "pca = PCA(n_components=0.95)  \n",
    "principal_components_spectrogram = pca.fit_transform(spectrogram_data_num)\n",
    "\n",
    "pca_df = pd.DataFrame(data=principal_components_spectrogram)\n",
    "\n",
    "print(f\"Number of components selected: {pca.n_components_}\")\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "pca_df['file_path'] = spectrogram_data['file_path'].values\n",
    "pca_df['animal_label'] = pca_df['file_path'].apply(extract_label)\n",
    "pca_df = pca_df.drop(columns=[\"file_path\"]) \n",
    "print(pca_df.head())\n",
    "\n",
    "# Plot the cumulative explained variance\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Cumulative Explained Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label=\"95% Explained Variance\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ecfe8",
   "metadata": {},
   "source": [
    "### Perform KMeans clustering on the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc49631-1469-457a-989b-5f9b9a94b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "pca_df['cluster'] = kmeans.fit_predict(principal_components_spectrogram)\n",
    "\n",
    "# Create a Contingency Matrix\n",
    "contingency_matrix_k_means = pd.crosstab(pca_df['animal_label'], pca_df['cluster'])\n",
    "\n",
    "# Ensure all clusters and labels are represented\n",
    "expected_clusters = range(kmeans.n_clusters)\n",
    "expected_labels = sorted(pca_df['animal_label'].unique())\n",
    "contingency_matrix_k_means = contingency_matrix_k_means.reindex(index=expected_labels, columns=expected_clusters, fill_value=0)\n",
    "print(contingency_matrix_k_means)\n",
    "\n",
    "plot_and_compute_metrics(pca_df, contingency_matrix_k_means, expected_clusters, expected_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc664a56",
   "metadata": {},
   "source": [
    "### Perform Spectral clustering on the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94257b-e8c4-49e1-9dd4-10c2201d25e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Spectral Clustering\n",
    "spectral_clustering = SpectralClustering(\n",
    "    n_clusters=10, \n",
    "    affinity='nearest_neighbors', \n",
    "    random_state=42, \n",
    "    assign_labels='kmeans'\n",
    ")\n",
    "pca_df['cluster'] = spectral_clustering.fit_predict(principal_components_spectrogram)\n",
    "\n",
    "# Create a Contingency Matrix\n",
    "contingency_matrix_spectral = pd.crosstab(pca_df['animal_label'], pca_df['cluster'])\n",
    "\n",
    "# Ensure all clusters and labels are represented\n",
    "expected_clusters = range(10) \n",
    "expected_labels = sorted(pca_df['animal_label'].unique())\n",
    "contingency_matrix_spectral = contingency_matrix_spectral.reindex(index=expected_labels, columns=expected_clusters, fill_value=0)\n",
    "print(contingency_matrix_spectral)\n",
    "\n",
    "plot_and_compute_metrics(pca_df, contingency_matrix_spectral, expected_clusters, expected_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da226ca3",
   "metadata": {},
   "source": [
    "### Perform GMM clustering on the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9770ab2-c57b-4e46-9d21-8bc47b1ce82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Gaussian Mixture Model (GMM)\n",
    "gmm = GaussianMixture(n_components=10, random_state=42)  \n",
    "pca_df['cluster'] = gmm.fit_predict(principal_components_spectrogram)\n",
    "\n",
    "# Create a Contingency Matrix\n",
    "contingency_matrix_gmm = pd.crosstab(pca_df['animal_label'], pca_df['cluster'])\n",
    "\n",
    "# Ensure all clusters and labels are represented\n",
    "expected_clusters = range(gmm.n_components)\n",
    "expected_labels = sorted(pca_df['animal_label'].unique())\n",
    "contingency_matrix_gmm = contingency_matrix_gmm.reindex(index=expected_labels, columns=expected_clusters, fill_value=0)\n",
    "print(contingency_matrix_gmm)\n",
    "\n",
    "plot_and_compute_metrics(pca_df, contingency_matrix_gmm, expected_clusters, expected_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb22c8",
   "metadata": {},
   "source": [
    "### Perform Self-Organizing Maps clustering on the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65330c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data \n",
    "scaler = StandardScaler()\n",
    "animal_scaled = scaler.fit_transform(principal_components_spectrogram) \n",
    "\n",
    "# Initialize the SOM\n",
    "som = MiniSom(x=10, y=10, input_len=animal_scaled.shape[1], sigma=1.0, learning_rate=0.5, random_seed=42)\n",
    "\n",
    "# Train the SOM\n",
    "som.train(animal_scaled, 1000, verbose=True)\n",
    "\n",
    "# Plot the SOM's U-matrix\n",
    "# U-matrix visualizes the distance between the neurons in the SOM grid. Larger distances mean dissimilarity.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"U-Matrix of SOM\")\n",
    "sns.heatmap(som.distance_map().T, cmap='coolwarm', cbar=False)\n",
    "plt.show()\n",
    "\n",
    "# Plot the data points on the SOM grid\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(som.distance_map().T, cmap='coolwarm', cbar=False)\n",
    "\n",
    "# Assign each data point to a neuron (winning node)\n",
    "win_map = som.win_map(animal_scaled)\n",
    "\n",
    "# We can use the same colors to show where the data points map on the SOM grid\n",
    "for label in range(len(animal_scaled)):\n",
    "    x, y = som.winner(animal_scaled[label])\n",
    "    plt.text(x, y, str(label), color='black', fontsize=10)\n",
    "\n",
    "plt.title('Data Points Mapped to SOM Grid')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the SOM clusters\n",
    "# Cluster the data points by grouping based on their winning node\n",
    "# You can assign each point a cluster label (i.e., the coordinates of the winning node)\n",
    "labels_som = np.array([som.winner(x) for x in animal_scaled])\n",
    "\n",
    "# Convert coordinates to cluster labels\n",
    "cluster_labels = np.array([f'Cluster_{x[0]}_{x[1]}' for x in labels_som])\n",
    "\n",
    "pca_df['SOM_cluster'] = cluster_labels\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=animal_scaled[:, 0], y=animal_scaled[:, 1], hue=pca_df['SOM_cluster'], palette='Set1', s=100)\n",
    "plt.title(\"Clustering on SOM Grid\")\n",
    "plt.show()\n",
    "\n",
    "# Clustering performance metrics\n",
    "ari = adjusted_rand_score(pca_df['animal_label'], pca_df['SOM_cluster'])\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.2f}\")\n",
    "\n",
    "nmi = normalized_mutual_info_score(pca_df['animal_label'], pca_df['SOM_cluster'])\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi:.2f}\")\n",
    "\n",
    "homogeneity = homogeneity_score(pca_df['animal_label'], pca_df['SOM_cluster'])\n",
    "print(f\"Homogeneity: {homogeneity:.2f}\")\n",
    "\n",
    "completeness = completeness_score(pca_df['animal_label'], pca_df['SOM_cluster'])\n",
    "print(f\"Completeness: {completeness:.2f}\")\n",
    "\n",
    "v_measure = v_measure_score(pca_df['animal_label'], pca_df['SOM_cluster'])\n",
    "print(f\"V-Measure: {v_measure:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8076e703",
   "metadata": {},
   "source": [
    "### Results of Spectrogram Feature Extraction Clustering Results\n",
    "\n",
    "put evaluation metrics for each model here in a table just for easy viewing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c3abf",
   "metadata": {},
   "source": [
    "# Clustering Results\n",
    "\n",
    "for each method of feature extraction talk about the results of the clustering algorithms (compare them to each other for the given feature extraction method). also talk about for a given clustering method why it works better or worse for a given feature extraction method. talk about how clustering didn't seem to work too well for either feature extraction method and it didn't seem likely that we'd get good results classifying from the clusters. we pivoted to a cnn feature extractor and various classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d7d44",
   "metadata": {},
   "source": [
    "# Using cnn as feature extractor and various classifiers (word better)\n",
    "we used mobile net pretrained (talk about what mobile net is and why we thought this would be good)\n",
    "\n",
    "For the feature extraction model, we're using the MobileNet convolutional neural network (CNN). This model is an efficient CNN pre-trained on the ImageNet dataset. It's optimized for small datasets like the one we are using in this study. We add in a GlobalAveragePooling2D layer as well as a Dense layer for the feature extraction layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716c091",
   "metadata": {},
   "source": [
    "#### Loading and preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c339990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio_file(file_path, target_shape=(224, 224)):\n",
    "\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        if y is None or len(y) == 0:\n",
    "            raise ValueError(f\"Audio data is empty for file: {file_path}\")\n",
    "        \n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=128)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        resized_spec = resize(mel_spec_db, target_shape, mode='constant')\n",
    "        \n",
    "        rgb_spec = np.stack([resized_spec] * 3, axis=-1) / 255.0 \n",
    "\n",
    "        return rgb_spec\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_directory_to_dataframe(parent_directory, target_shape=(224, 224)):\n",
    "\n",
    "    data, labels = [], []\n",
    "    for root, _, files in os.walk(parent_directory):\n",
    "        label = os.path.basename(root)  \n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    spectrogram = preprocess_audio_file(file_path, target_shape)\n",
    "                    if spectrogram is not None:\n",
    "                        data.append(spectrogram)\n",
    "                        labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping file {file_path} due to error: {e}\")\n",
    "\n",
    "    print(f\"Processed {len(data)} files successfully.\")\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "parent_directory = \"Animal_Sounds\"  \n",
    "\n",
    "print(\"Processing audio files...\")\n",
    "X, y = process_directory_to_dataframe(parent_directory)\n",
    "\n",
    "print(\"Encoding labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "print(\"Splitting data into train and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec9b2f",
   "metadata": {},
   "source": [
    "## logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699241c0",
   "metadata": {},
   "source": [
    "### Implement and Train the logistic regression classifier\n",
    "discuss what this cell does as a process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e14d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNet model\n",
    "print(\"Loading pre-trained MobileNet model...\")\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a feature extractor from the pre-trained model\n",
    "feature_extractor_model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu')  \n",
    "])\n",
    "\n",
    "# Data augmentation and training\n",
    "datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Extract features for training and testing data\n",
    "print(\"Extracting features using the pre-trained MobileNet model...\")\n",
    "X_train_features = feature_extractor_model.predict(X_train)\n",
    "X_test_features = feature_extractor_model.predict(X_test)\n",
    "\n",
    "# Train Logistic Regression Classifier\n",
    "print(\"Training the Logistic Regression classifier...\")\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,  \n",
    "    random_state=42,\n",
    "    multi_class='multinomial',  \n",
    "    solver='lbfgs'              \n",
    ")\n",
    "lr_model.fit(X_train_features, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Evaluate the Model\n",
    "print(\"Evaluating the Logistic Regression classifier...\")\n",
    "y_pred = lr_model.predict(X_test_features)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab05a67",
   "metadata": {},
   "source": [
    "### Logistic Regression Analysis\n",
    "\n",
    "talk about how it did and why we think it did well or poorly given our dataset and the strengths and weaknesses of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace3507f",
   "metadata": {},
   "source": [
    "## RNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d05ecc6",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "talk about what this cell does as a process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e778ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNet model\n",
    "print(\"Loading pre-trained MobileNet model...\")\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a feature extractor from the pre-trained model\n",
    "feature_extractor_model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu')  # Feature embedding layer\n",
    "])\n",
    "\n",
    "# Data augmentation and training\n",
    "datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Extract features for training and testing data\n",
    "print(\"Extracting features using the pre-trained MobileNet model...\")\n",
    "X_train_features = feature_extractor_model.predict(X_train)\n",
    "X_test_features = feature_extractor_model.predict(X_test)\n",
    "\n",
    "# Reshape features for RNN input\n",
    "timesteps = 1  \n",
    "X_train_rnn = X_train_features.reshape(X_train_features.shape[0], timesteps, -1)\n",
    "X_test_rnn = X_test_features.reshape(X_test_features.shape[0], timesteps, -1)\n",
    "\n",
    "# Build the RNN model\n",
    "print(\"Building the CNN + RNN hybrid model...\")\n",
    "\n",
    "rnn_model = Sequential([\n",
    "    TimeDistributed(Dense(128, activation='relu'), input_shape=(timesteps, X_train_rnn.shape[2])),\n",
    "    LSTM(128, return_sequences=False),  # LSTM for temporal modeling\n",
    "    Dropout(0.5),\n",
    "    Dense(y_train.shape[1], activation='softmax')  # Classification layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = AdamW(learning_rate=0.001, weight_decay=1e-4)\n",
    "rnn_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c77fad1",
   "metadata": {},
   "source": [
    "### Train the RNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44bb606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training the CNN + RNN hybrid model...\")\n",
    "\n",
    "# Early Stopping to reduce overfitting by stopping training when validation loss does not change for 5 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Learning Rate Scheduler to reduce LR when validation loss plateaus\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    factor=0.5,          # Reduce LR by a factor of 0.5\n",
    "    patience=3,          # Wait 3 epochs before reducing LR\n",
    "    verbose=1,\n",
    "    min_lr=1e-6          # Minimum learning rate\n",
    ")\n",
    "\n",
    "history = rnn_model.fit(\n",
    "    X_train_rnn, y_train,\n",
    "    validation_data=(X_test_rnn, y_test),\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping,lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = rnn_model.evaluate(X_test_rnn, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "y_pred_probs = rnn_model.predict(X_test_rnn)  # Predicted probabilities\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Predicted classes\n",
    "y_true = np.argmax(y_test, axis=1)  # True classes\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14795f77",
   "metadata": {},
   "source": [
    "### RNN Analysis\n",
    "\n",
    "talk about how it did and why we think it did well or poorly given our dataset and the strengths and weaknesses of rnns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3fcbe9",
   "metadata": {},
   "source": [
    "## SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86e157",
   "metadata": {},
   "source": [
    "### Feature Extraction and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb30587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNet model\n",
    "print(\"Loading pre-trained MobileNet model...\")\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a feature extractor from the pre-trained model\n",
    "feature_extractor_model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu')  # Feature embedding layer\n",
    "])\n",
    "\n",
    "# Data augmentation and training\n",
    "datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Extract features for training and testing data\n",
    "print(\"Extracting features using the pre-trained MobileNet model...\")\n",
    "X_train_features = feature_extractor_model.predict(X_train)\n",
    "X_test_features = feature_extractor_model.predict(X_test)\n",
    "\n",
    "# Train-Test Split after PCA\n",
    "print(\"Training the SVM classifier...\")\n",
    "\n",
    "# Train SVM\n",
    "svm_classifier = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svm_classifier.fit(X_train_features, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Evaluate SVM\n",
    "print(\"Evaluating the SVM classifier...\")\n",
    "y_pred = svm_classifier.predict(X_test_features)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f26bd8",
   "metadata": {},
   "source": [
    "### SVM Analysis\n",
    "\n",
    "talk about how it did and why we think it did well or poorly given our dataset and the strengths and weaknesses of SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e18e70",
   "metadata": {},
   "source": [
    "## XGBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d3699",
   "metadata": {},
   "source": [
    "### Feature Extraction and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2499409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNet model\n",
    "print(\"Loading pre-trained MobileNet model...\")\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a feature extractor from the pre-trained model\n",
    "feature_extractor_model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu')  # Feature embedding layer\n",
    "])\n",
    "\n",
    "# Data augmentation and training\n",
    "datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Extract features for training and testing data\n",
    "print(\"Extracting features using the pre-trained MobileNet model...\")\n",
    "X_train_features = feature_extractor_model.predict(X_train)\n",
    "X_test_features = feature_extractor_model.predict(X_test)\n",
    "\n",
    "# Apply PCA\n",
    "print(\"Applying PCA to extracted features...\")\n",
    "pca = PCA(n_components=0.95) \n",
    "X_train_pca = pca.fit_transform(X_train_features)\n",
    "X_test_pca = pca.transform(X_test_features)\n",
    "\n",
    "# Check the number of components selected\n",
    "print(f\"Number of components selected by PCA: {pca.n_components_}\")\n",
    "\n",
    "# Train XGBoost Classifier\n",
    "print(\"Training the XGBoost classifier...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',  # Multiclass classification\n",
    "    num_class=y_train.shape[1],  # Number of classes\n",
    "    use_label_encoder=False,     # Suppress warnings\n",
    "    eval_metric='mlogloss',      # Log loss for multiclass\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_features, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Evaluate the Model\n",
    "print(\"Evaluating the XGBoost classifier...\")\n",
    "y_pred = xgb_model.predict(X_test_features)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59c8d0",
   "metadata": {},
   "source": [
    "### XGBoost Analysis\n",
    "\n",
    "talk about how it did and why we think it did well or poorly given our dataset and the strengths and weaknesses of XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f528ac2",
   "metadata": {},
   "source": [
    "## CNN as feature extractor and Various Classifiers results\n",
    "\n",
    "put it all together in a table for easy viewing. proabably accuracy as the metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2aac0b",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "discuss overall what we saw with all the different models and why some did better than others. also talk about the three methods of feature extraction and why we think the cnn feature extractor did the best. talk about how we could improve the models and what we would do differently if we had more time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d410a236",
   "metadata": {},
   "source": [
    "# Limitations\n",
    "\n",
    "compute power (could try bigger and more complex models with more compute), time (more time we could try different shit), dataset size (more data = more better)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550ec38",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "wrap everything up high level, basically restating the abstract in different words. talk about future directions with this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dada005",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "just cite some links to a paper about each model (like how it works or whatever) so it can look good cuz we'd have like 8 sources "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7ce04a-b766-439f-9542-764571998c86",
   "metadata": {},
   "source": [
    "# CNN Feature Extractor to SVM Classifier\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook displays the model I created to clasify animals by their sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1ab114-9fa6-4717-b655-f981b7f5b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation Packages\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import subprocess\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Keras Packages\n",
    "\n",
    "# Neural Network Models\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "# Layers and Parameters for CNN\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, GlobalAveragePooling2D, Attention\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# SVM \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Results Metrics and Plots\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff5336-adbe-49f6-aa7e-bdfba45b894c",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data is from two different datasets sources form GitHub and Kaggle. It includes .wav files from 12 different animals and has 875 observations. The data is preprocessed into a melspectrogram, then stored in a dataframe with the melspectrogram data and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c27193-ef77-4dde-9f3b-987dc028917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio_file(file_path, target_shape=(224, 224)):\n",
    "    \"\"\"\n",
    "    Converts a .wav file into a Mel Spectrogram, resizes it,\n",
    "    and normalizes pixel values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        # Validate audio data\n",
    "        if y is None or len(y) == 0:\n",
    "            raise ValueError(f\"Audio data is empty for file: {file_path}\")\n",
    "        \n",
    "        # Compute Mel Spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=128)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Resize to the target shape\n",
    "        resized_spec = resize(mel_spec_db, target_shape, mode='constant')\n",
    "        \n",
    "        # Normalize and convert to RGB\n",
    "        rgb_spec = np.stack([resized_spec] * 3, axis=-1) / 255.0  # Normalize to [0, 1]\n",
    "        return rgb_spec\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_directory_to_dataframe(parent_directory, target_shape=(224, 224)):\n",
    "    \"\"\"\n",
    "    Processes all .wav files in a directory and its subdirectories,\n",
    "    computes their Mel Spectrograms, and stores them in arrays.\n",
    "    \"\"\"\n",
    "    data, labels = [], []\n",
    "    for root, _, files in os.walk(parent_directory):\n",
    "        label = os.path.basename(root)  # Subdirectory name is the label\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # Preprocess the audio file\n",
    "                    spectrogram = preprocess_audio_file(file_path, target_shape)\n",
    "                    if spectrogram is not None:\n",
    "                        data.append(spectrogram)\n",
    "                        labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping file {file_path} due to error: {e}\")\n",
    "\n",
    "    print(f\"Processed {len(data)} files successfully.\")\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Path to the parent directory containing labeled subdirectories of .wav files\n",
    "parent_directory = \"Animal_Sounds\" # Replace with your dataset directory\n",
    "\n",
    "if os.path.exists(parent_directory):\n",
    "    print(f\"Directory exists: {parent_directory}\")\n",
    "else:\n",
    "    print(f\"Directory not found: {parent_directory}\")\n",
    "\n",
    "# Process audio files and labels\n",
    "print(\"Processing audio files...\")\n",
    "X, y = process_directory_to_dataframe(parent_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec31146-6caf-439c-897c-d8e1aa888cf4",
   "metadata": {},
   "source": [
    "## Encoding and Data Split\n",
    "\n",
    "This cell encodes the labels and splits the data into the train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b82525f-bcf2-40e5-9fcd-a06e70b65bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "print(\"Encoding labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Train-test split\n",
    "print(\"Splitting data into train and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff0c9a-bc0b-4328-85f4-88a6d7b709e2",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "For the feature extraction model, we're using the MobileNet convolutional neural network (CNN). This model is an efficient CNN pre-trained on the ImageNet dataset. It's optimized for small datasets like the one we are using in this study. We add in a GlobalAveragePooling2D layer as well as a Dense layer for the feature extraction layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73dcaf2-a1cb-4f40-a1cd-60775e10f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNet model\n",
    "print(\"Loading pre-trained MobileNet model...\")\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a feature extractor from the pre-trained model\n",
    "feature_extractor_model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu')  # Feature embedding layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b59f89d-d891-4525-b9d0-72078a0b4b54",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "In this cell we augment the data to combat overfitting of the classification model in the long run. Additional we extract the features in this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465b680f-dac1-4b31-9382-89558373d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and training\n",
    "datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Extract features for training and testing data\n",
    "print(\"Extracting features using the pre-trained MobileNet model...\")\n",
    "X_train_features = feature_extractor_model.predict(X_train)\n",
    "X_test_features = feature_extractor_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99dc78-212c-4b26-bd49-4edbeb12abb8",
   "metadata": {},
   "source": [
    "## Training the SVM\n",
    "\n",
    "Here we are using sklearn's SVM package, which we will train on the features extracted from the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d2f65-8cef-4845-8e7d-2dd35a90e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split after PCA\n",
    "print(\"Training the SVM classifier...\")\n",
    "\n",
    "# Train SVM\n",
    "svm_classifier = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svm_classifier.fit(X_train_features, np.argmax(y_train, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb2a1d-cff8-484a-8386-580fb212c095",
   "metadata": {},
   "source": [
    "## SVM Results\n",
    "\n",
    "Now we are evaluating the results from the SVM on the test set. Additionally, we're printing out a report of how well it performed along with a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc88987-2306-4e20-bc87-58bd615ab128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate SVM\n",
    "print(\"Evaluating the SVM classifier...\")\n",
    "y_pred = svm_classifier.predict(X_test_features)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f2722-2bf9-4e72-a31d-0b77d31ff691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
